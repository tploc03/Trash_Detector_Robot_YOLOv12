import cv2
import time
import torch
from PyQt6.QtCore import QThread, pyqtSignal
from PyQt6.QtGui import QImage
from ultralytics import YOLO

class VideoThread(QThread):
    change_pixmap_signal = pyqtSignal(QImage)
    ai_results_signal = pyqtSignal(dict)
    fps_signal = pyqtSignal(int)
    
    def __init__(self, cam_ip, model_path):
        super().__init__()
        self.cam_ip = cam_ip
        self.model_path = model_path
        self.model = None
        self.confidence = 0.7
        self.running = True
        self.cap = None
        self.fps_counter = 0
        self.last_fps_time = 0
        
    def run(self):
        try:
            # Load model
            print("ü§ñ Loading YOLO model...")
            if torch.cuda.is_available():
                self.model = YOLO(self.model_path).to(0)  # GPU
                print("‚úì Model loaded on GPU")
            else:
                self.model = YOLO(self.model_path)  # CPU
                print("‚úì Model loaded on CPU")
            
            # M·ªü camera
            self.cap = cv2.VideoCapture(self.cam_ip)
            
            # S·ª¨A: C√†i ƒë·∫∑t buffer nh·ªè ƒë·ªÉ tr√°nh delay
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            
            if not self.cap.isOpened():
                print(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi camera: {self.cam_ip}")
                return
            
            print(f"‚úì Camera ƒë√£ k·∫øt n·ªëi: {self.cam_ip}")
            
            last_frame_time = time.time()
            frame_count = 0
            
            while self.running:
                ret, frame = self.cap.read()
                
                if not ret:
                    print("‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera")
                    # Th·ª≠ k·∫øt n·ªëi l·∫°i
                    self.cap.release()
                    time.sleep(0.5)
                    self.cap = cv2.VideoCapture(self.cam_ip)
                    continue
                
                # S·ª¨A: Gi·∫£m k√≠ch th∆∞·ªõc frame ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n
                frame = cv2.resize(frame, (640, 480))
                
                # Inference
                annotated_frame = frame.copy()
                try:
                    results = self.model(frame, conf=self.confidence, verbose=False)
                    detections = []
                    
                    if results and len(results) > 0:
                        boxes = results[0].boxes
                        for box in boxes:
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            conf = float(box.conf[0])
                            cls = int(box.cls[0])
                            label = self.model.names[cls]
                            
                            detections.append({
                                'bbox': (x1, y1, x2, y2),
                                'conf': conf,
                                'class': cls,
                                'label': label
                            })
                            
                            # V·∫Ω bbox
                            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                            text = f"{label} {conf:.2f}"
                            cv2.putText(annotated_frame, text, (x1, y1-10), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                    
                    if detections:
                        self.ai_results_signal.emit({
                            'detections': detections,
                            'frame_shape': frame.shape
                        })
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói inference: {e}")
                
                # Convert to QImage
                rgb_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
                h, w, ch = rgb_frame.shape
                bytes_per_line = ch * w
                convert_to_Qt_format = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)
                p = convert_to_Qt_format.scaledToWidth(640)
                self.change_pixmap_signal.emit(p)
                
                # FPS counter
                frame_count += 1
                current_time = time.time()
                if current_time - last_frame_time >= 1:
                    fps = frame_count / (current_time - last_frame_time)
                    self.fps_signal.emit(int(fps))
                    frame_count = 0
                    last_frame_time = current_time
                
        except Exception as e:
            print(f"‚ùå Video Thread Error: {e}")
        finally:
            if self.cap:
                self.cap.release()
    
    def update_source(self, cam_ip):
        """C·∫≠p nh·∫≠t ngu·ªìn camera"""
        self.cam_ip = cam_ip
        if self.cap:
            self.cap.release()
        self.cap = cv2.VideoCapture(self.cam_ip)
        print(f"üîÑ Camera updated: {cam_ip}")
    
    def update_conf(self, conf):
        """C·∫≠p nh·∫≠t confidence threshold"""
        self.confidence = conf
        print(f"üîß Confidence updated: {conf}")
    
    def stop(self):
        self.running = False
        self.wait()
