# video.py - FINAL FIXED AATTN ERROR
import cv2
import numpy as np
from PyQt6.QtCore import QThread, pyqtSignal
from PyQt6.QtGui import QImage
from ultralytics import YOLO
import time
import torch
import types

# --- HÃ€M VÃ Lá»–I YOLOv12 ---
def fix_aattn_compat(m):
    """Sá»­a lá»—i thiáº¿u thuá»™c tÃ­nh 'qkv' trong module AAttn cá»§a YOLOv12"""
    try:
        # Láº¥y model pytorch gá»‘c tá»« wrapper cá»§a Ultralytics
        model_to_scan = m.model if hasattr(m, 'model') else m
        
        for mod in model_to_scan.modules():
            # TÃ¬m cÃ¡c module tÃªn lÃ  AAttn
            if mod.__class__.__name__ == 'AAttn':
                # Náº¿u thiáº¿u hÃ m qkv nhÆ°ng cÃ³ qk vÃ  v -> Táº¡o hÃ m qkv giáº£ láº­p
                if not hasattr(mod, 'qkv') and hasattr(mod, 'qk') and hasattr(mod, 'v'):
                    def _qkv(self, x):
                        qk_out = self.qk(x)
                        v_out = self.v(x)
                        return torch.cat([qk_out, v_out], dim=1)
                    
                    # GÃ¡n hÃ m má»›i vÃ o module (Monkey patching)
                    mod.qkv = types.MethodType(_qkv, mod)
                    
        print("âœ… Applied YOLOv12 AAttn compatibility fix.")
    except Exception as e:
        print(f"âš ï¸ Warning: Could not apply AAttn fix: {e}")

class VideoThread(QThread):
    change_pixmap_signal = pyqtSignal(QImage)
    ai_results_signal = pyqtSignal(dict)
    fps_signal = pyqtSignal(int)

    def __init__(self, stream_url, model_path):
        super().__init__()
        self.stream_url = stream_url
        self.model_path = model_path
        self._run_flag = True
        self.ai_enabled = False
        self.confidence = 0.25
        self.reconnect_requested = False
        
        self.frame_count = 0
        self.fps = 0
        self.last_fps_time = time.time()
        self.process_every_n_frames = 2  # ðŸ†• Giáº£m tá»« 4 -> 2 Ä‘á»ƒ detection nhanh hÆ¡n (cháº¡y má»—i 2 frame)
        self.ai_frame_counter = 0
        self.detection_count = 0  # ðŸ†• Debug: Äáº¿m sá»‘ detection Ä‘Ã£ cháº¡y

    def update_source(self, url):
        if url != self.stream_url:
            print(f"ðŸ”„ Setting new URL: {url}")
            self.stream_url = url
            self.reconnect_requested = True

    def update_conf(self, conf):
        self.confidence = conf
        
    def set_ai_mode(self, enabled):
        self.ai_enabled = enabled
        # Chá»‰ load model khi cáº§n thiáº¿t (khi báº­t Auto Mode)
        if enabled and not hasattr(self, 'model'):
            print(f"ðŸ”„ Loading YOLO model from {self.model_path}...")
            try:
                self.model = YOLO(self.model_path)
                
                # --- Gá»ŒI HÃ€M Sá»¬A Lá»–I NGAY SAU KHI LOAD ---
                fix_aattn_compat(self.model) 
                # -----------------------------------------
                
                print("âœ… Model loaded successfully")
                print(f"ðŸŽ¯ AI Detection ENABLED - Running on every {self.process_every_n_frames} frames")
            except Exception as e:
                print(f"âŒ Model Error: {e}")
                self.ai_enabled = False # Táº¯t AI náº¿u load lá»—i
        elif not enabled:
            print("â¸ï¸  AI Detection DISABLED")

    def run(self):
        print(f"ðŸš€ Video Thread Starting with: {self.stream_url}")
        cap = cv2.VideoCapture(self.stream_url)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        while self._run_flag:
            if self.reconnect_requested:
                print(f"ðŸ”„ Reconnecting to NEW IP: {self.stream_url} ...")
                if cap.isOpened():
                    cap.release()
                time.sleep(0.5)
                cap = cv2.VideoCapture(self.stream_url)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                self.reconnect_requested = False

            ret, frame = cap.read()
            
            if not ret:
                if int(time.time()) % 2 == 0:
                    print("âš ï¸ No Frame. Check IP or Wifi.")
                self.msleep(500)
                continue
            
            # Resize Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½
            h, w = frame.shape[:2]
            if w > 640:
                scale = 640 / w
                frame = cv2.resize(frame, (640, int(h * scale)))
            
            # --- AI Logic ---
            if self.ai_enabled and hasattr(self, 'model'):
                self.ai_frame_counter += 1
                if self.ai_frame_counter % self.process_every_n_frames == 0:
                    try:
                        # Dá»± Ä‘oÃ¡n
                        results = self.model.predict(
                            frame, 
                            conf=self.confidence, 
                            verbose=False, 
                            device='cpu', 
                            max_det=5
                        )
                        
                        detections = []
                        if results:
                            # Váº½ box vÃ  láº¥y thÃ´ng tin
                            for box in results[0].boxes:
                                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                                conf = float(box.conf[0])
                                cls = int(box.cls[0])
                                label = results[0].names[cls]
                                center_x = int((x1 + x2) / 2)
                                
                                detections.append({
                                    'label': label, 
                                    'conf': conf, 
                                    'center_x': center_x, 
                                    'bbox': [int(x1), int(y1), int(x2), int(y2)]
                                })
                                
                                # Váº½ trá»±c tiáº¿p lÃªn frame
                                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                                cv2.putText(frame, f"{label} {conf:.2f}", (int(x1), int(y1)-10), 
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                                          
                        if detections:
                            self.detection_count += 1
                            # ðŸ†• Log detection má»™t láº§n má»—i 30 frame
                            if self.detection_count % 30 == 0:
                                print(f"ðŸ” Detection #{self.detection_count}: Found {len(detections)} object(s)")
                            self.ai_results_signal.emit({'detections': detections})
                            
                    except Exception as e:
                        print(f"AI Error: {e}")

            # Convert to Qt Image
            rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)
            self.change_pixmap_signal.emit(qt_image)
            
            # FPS Calculation
            self.frame_count += 1
            if time.time() - self.last_fps_time >= 1.0:
                self.fps = self.frame_count
                self.fps_signal.emit(self.fps)
                self.frame_count = 0
                self.last_fps_time = time.time()

            self.msleep(1)

        cap.release()
        print("ðŸ›‘ Video Thread Stopped")

    def stop(self):
        self._run_flag = False
        if not self.wait(2000):
            self.terminate()